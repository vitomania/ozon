{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQwlunyE86kE"
   },
   "source": [
    "# 1 практическое задание. Обучение полносвязной нейронной сети.\n",
    "\n",
    "**ФИО**: Кожемяк Виталий Васильевич\n",
    "\n",
    "**Дедлайн**: 29 сентября 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:41:52.599041Z",
     "start_time": "2020-09-29T20:41:52.498128Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UyCGfNgA86kF"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHcumwuF86kL"
   },
   "source": [
    "# Реализация нейронной сети (15 баллов)\n",
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры, [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:41:54.186940Z",
     "start_time": "2020-09-29T20:41:54.180484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SiqeRVcM86kM"
   },
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4IoM_pX86kQ"
   },
   "source": [
    "\n",
    "# Слой нелинейности ReLU\n",
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote \n",
    "$$\n",
    "ReLU(\\vec{x}) = ReLU(x_1, \\ldots, x_n) = (\\max(0, x_1), \\ldots, \\max(0, x_n)),\n",
    "$$ where $n$ - number of inputs.\n",
    "Thus,\n",
    "$$ \n",
    "\\dfrac{\\partial ReLU(\\vec{x})}{\\partial \\vec{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial \\max(0, x_1)}{\\partial x_1} & 0 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
    "0 & 0 & \\ldots & \\dfrac{\\partial \\max(0, x_n)}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\\dfrac{\\partial\\max(0, x_i)}{\\partial x_i} = \n",
    "\\begin{cases}\n",
    "1, & x_i > 0,\\\\\n",
    "0, & x_i \\leqslant 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "where $i = {1, \\ldots, n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:41:55.924953Z",
     "start_time": "2020-09-29T20:41:55.918686Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "y09ZVCsT86kT"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        self.input = input\n",
    "        return np.maximum(input,  0)\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        output = np.zeros(grad_output.shape)\n",
    "        mask = ~(self.input <= 0)\n",
    "        d_layer_d_input = 1 * mask\n",
    "        output = grad_output * d_layer_d_input\n",
    "        \n",
    "        return output, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojTR4GFd86kY"
   },
   "source": [
    "# Полносвязный слой\n",
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига.\n",
    "\n",
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:41:57.295021Z",
     "start_time": "2020-09-29T20:41:57.287868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YbN5JOc886kZ"
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        output = np.dot(input, self.weights) + self.biases\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        self.grad_b = np.ones(self.biases.shape[0])\n",
    "        self.grad_W = np.dot(self.input.T, grad_output)\n",
    "        return np.dot(grad_output, self.weights.T), list(np.ravel(self.grad_W)) + list(np.ravel(self.grad_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2naG3La86kd"
   },
   "source": [
    "# Проверка градиента\n",
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:41:58.589176Z",
     "start_time": "2020-09-29T20:41:58.583576Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TRNJl7eK86ke"
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    x has dimension[batch_size, input_size]\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        # evaluate function at x+h\n",
    "        oldval = x[:, i].copy()\n",
    "        x[:, i] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[:, i] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[:, i] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[:, i] = (fxph - fxmh) / (2 * h) # the slope\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0lA0Fpv86kj"
   },
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:00.262149Z",
     "start_time": "2020-09-29T20:42:00.256085Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4TaePvno86kj"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "grads = (~(x <= 0)) * 1\n",
    "f = lambda x: np.sum(np.maximum(x,  0), axis=1)\n",
    "numeric_grads = eval_numerical_gradient(f, x, h=0.00001)\n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHyw38jB86ko"
   },
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:01.258447Z",
     "start_time": "2020-09-29T20:42:01.249990Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0r0vZ1KY86kp"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "l = Dense(12, 32)\n",
    "\n",
    "f = lambda x: np.sum(x @ l.weights + l.biases, axis=1)\n",
    "grads = np.ones(x.shape) * l.weights.T.sum(axis=0)\n",
    "numeric_grads = eval_numerical_gradient(f, x, h=0.00001)\n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dyhDg0D86kt"
   },
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArL0HLGH86ku"
   },
   "source": [
    "Реализация softmax-слоя и функции потерь\n",
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$В этом случае удобно оптимизировать логарифм правдоподобия:$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$В таком виде ее удобно реализовывать.\n",
    "\n",
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде. Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:03.599622Z",
     "start_time": "2020-09-29T20:42:03.522897Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "g27f9NI186ku"
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:03.785071Z",
     "start_time": "2020-09-29T20:42:03.776971Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xSV3XD0N86ky"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        output = (\n",
    "            input\n",
    "            - np.max(input, axis=1, keepdims=True) \n",
    "            - logsumexp(input - np.max(input, axis=1, keepdims=True), axis=1).reshape(-1, 1)\n",
    "        )\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        softmax = np.exp(\n",
    "            self.input\n",
    "            - np.max(self.input, axis=1, keepdims=True) \n",
    "            - logsumexp(self.input - np.max(self.input, axis=1, keepdims=True), axis=1, keepdims=True)\n",
    "        )\n",
    "        output = grad_output - (softmax * grad_output.sum(axis=1, keepdims=True))\n",
    "        return output, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Sn2M_Q086k2"
   },
   "source": [
    "# Dropout\n",
    "Реализуйте слой Dropout. Сравните обучение сети из большого числа слоёв при использовании Dropout и без его использования (предварительно подберите адекватный параметр p). Сделайте выводы. Используя метод оптимизации из первого бонусного пункта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:05.115609Z",
     "start_time": "2020-09-29T20:42:05.110468Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qCLECy1y86k3"
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p):\n",
    "        self.params = []\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Randomly  drops connections between nurons.\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        ### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyPPzkCI86k7"
   },
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:06.609175Z",
     "start_time": "2020-09-29T20:42:06.604036Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "x0lUQZIN86k8"
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    return (np.where(target == 1, -activations, 0)).sum()\n",
    "    \n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    return np.where(target == 1, -1 / np.exp(activations), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8pR6Tcl86lA"
   },
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wLJgw1A86lB"
   },
   "source": [
    "# Загрузка данных\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:09.206543Z",
     "start_time": "2020-09-29T20:42:08.978800Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OVJvVXXQ86lB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:09.639039Z",
     "start_time": "2020-09-29T20:42:09.208917Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bz8eGZ3r86lF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:09.724089Z",
     "start_time": "2020-09-29T20:42:09.641804Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3QnpueaU86lL"
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:09.731484Z",
     "start_time": "2020-09-29T20:42:09.726471Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "f596OpXk86lO"
   },
   "outputs": [],
   "source": [
    "def encode(label):\n",
    "    batch_size = len(label)\n",
    "    res = np.zeros((batch_size, 10))\n",
    "    res[np.arange(batch_size), label - 1] = 1\n",
    "    return res\n",
    "\n",
    "y = encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:09.801782Z",
     "start_time": "2020-09-29T20:42:09.795221Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xfxXwllM86lS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797, 10))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXyzDFMH86lW"
   },
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:11.427870Z",
     "start_time": "2020-09-29T20:42:11.404986Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4vYMOr0H86lW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:11.650298Z",
     "start_time": "2020-09-29T20:42:11.644597Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "k0nYXVTG86la"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:12.254965Z",
     "start_time": "2020-09-29T20:42:12.248987Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "w-8Ee-d186ld"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmzKDpyE86lg"
   },
   "source": [
    "# Сборка и обучение нейронной сети (5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPqlZfj_86lg"
   },
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:13.827327Z",
     "start_time": "2020-09-29T20:42:13.821597Z"
    },
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "Sf2sNDJN86lh"
   },
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    network = []\n",
    "    hidden_layers_size = 32\n",
    "    network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "    network.append(ReLU())\n",
    "    network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network.append(ReLU())\n",
    "    network.append(Dense(hidden_layers_size, 10))\n",
    "    network.append(Softmax())\n",
    "    return network\n",
    "\n",
    "network = create_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltomU8p886lk"
   },
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:15.176413Z",
     "start_time": "2020-09-29T20:42:15.172409Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SV6uAjR286ll"
   },
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    output = X.copy()\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2nGs4Rq86lo"
   },
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:17.066058Z",
     "start_time": "2020-09-29T20:42:17.062445Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "94lKImJt86lo"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-ffcJkz86lw"
   },
   "source": [
    "\n",
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:18.432674Z",
     "start_time": "2020-09-29T20:42:18.427259Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8tm-JrYb86lw"
   },
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uNWIOxe86lz"
   },
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:19.763838Z",
     "start_time": "2020-09-29T20:42:19.758549Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5iq4nkWu86l0"
   },
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    set_weights(weights, network)\n",
    "    \n",
    "    output = X.copy()\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "        \n",
    "    loss = crossentropy(output, y)\n",
    "    grad = grad_crossentropy(output, y)\n",
    "    \n",
    "    final_grad = []\n",
    "    for i in range(len(network) - 1, -1, -1):\n",
    "        grad, g = network[i].backward(grad)\n",
    "        final_grad += g\n",
    "        \n",
    "    return loss, final_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHJmt1pD86l3"
   },
   "source": [
    "Теперь мы готовы обучать нашу нейросеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:22.654724Z",
     "start_time": "2020-09-29T20:42:22.650242Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Lb_t8iaz86l3"
   },
   "outputs": [],
   "source": [
    "network = create_network()\n",
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:23.591011Z",
     "start_time": "2020-09-29T20:42:23.481645Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XTYD1r2P86l7"
   },
   "outputs": [],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:23.925837Z",
     "start_time": "2020-09-29T20:42:23.920210Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "b_4bFVQp86mC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:24.903620Z",
     "start_time": "2020-09-29T20:42:24.897658Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3IL7UoCx86mE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:25.564202Z",
     "start_time": "2020-09-29T20:42:25.557883Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "djipcHJ786mH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02131534,  0.00059992, -0.01097774, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxF7dSlv86mK"
   },
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test. Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:27.818139Z",
     "start_time": "2020-09-29T20:42:27.815166Z"
    }
   },
   "outputs": [],
   "source": [
    "set_weights(res[\"x\"], network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:29.983854Z",
     "start_time": "2020-09-29T20:42:29.975620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3101.635865041483"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossentropy(predict(network, X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:31.276024Z",
     "start_time": "2020-09-29T20:42:31.267687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036.1547012147862"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossentropy(predict(network, X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzuOHEPY86mO"
   },
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:33.809952Z",
     "start_time": "2020-09-29T20:42:33.800764Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BWx3rls786mP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        set_weights(weights, network)\n",
    "        \n",
    "        \n",
    "        output = self.X_train\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "            \n",
    "        y_pred = np.argmax(np.exp(output), axis=1) + 1\n",
    "        self.train_acc.append(accuracy_score(y_pred, self.y_train))\n",
    "        \n",
    "        output = self.X_test\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "            \n",
    "        y_pred = np.argmax(np.exp(output), axis=1) + 1\n",
    "        self.train_acc.append(accuracy_score(y_pred, self.y_test))\n",
    "        \n",
    "        \n",
    "        self.test_acc.append(accuracy_score(y_pred))\n",
    "        if print:\n",
    "            print(\n",
    "                \"train_acc = {}, test_acc = {}\".format(\n",
    "                self.train_acc[-1],\n",
    "                self.test_acc[-1]\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T20:42:34.901409Z",
     "start_time": "2020-09-29T20:42:34.808172Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Omd3wRNE86mS"
   },
   "outputs": [],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40YCwfWI86mV"
   },
   "source": [
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSXra9Ub86mV"
   },
   "outputs": [],
   "source": [
    "plt.plot(cb.train_acc, label=\"train acc\")\n",
    "plt.plot(cb.test_acc, label=\"test acc\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keGygn6X86mY"
   },
   "source": [
    "Эксперименты с числом слоев (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HqHg3bD86mc"
   },
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyx-Jf8R86mg"
   },
   "outputs": [],
   "source": [
    "accs_test = np.zeros((5, 5))\n",
    "accs_train = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uC_ygVJV86mj"
   },
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ik2G2TbG86mp"
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyUp3bX_86mp"
   },
   "outputs": [],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAztSPvw86mt"
   },
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "\n",
    "Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?\n",
    "* Несколько фрагментов кода в задании написаны на основе материалов курса по глубинному обучению на ФКН НИУ ВШЭ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3vcONKQ86mu"
   },
   "source": [
    "# Бонусная часть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kuhxy7aW86mu"
   },
   "source": [
    "## Реализация метода оптимизации (2 балла).\n",
    "Реализуйте сами метод оптимизации (аналог функции minimize) для рассмотренной выше архитектуры. В качестве метода оптимизации используйте SGD + momentum. Продемонстрируйте правильную работу метода оптимизации, сравните его работы с LBFGS-B. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeCyLmi_86mv"
   },
   "source": [
    "# 1D Сверточный слой (2 балла)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vr4OtHvp86mw"
   },
   "source": [
    "Сверткой сигнала $\\mathbf{x}$ ядром  $\\mathbf{k}$ называется переобразование:\n",
    "$$\n",
    " y_i = (\\mathbf{x} \\circ \\mathbf{k})_i = \\sum \\limits_{j=i-t}^{i+t} x_j \\cdot k_{j-i+t}, \\; |\\mathbf{k}| = 2t + 1\n",
    "$$\n",
    "\n",
    "Найти явные формулы для:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf y}{\\partial \\mathbf x}, \\; \\frac{\\partial \\mathbf y}{\\partial \\mathbf k}\n",
    "$$\n",
    "\n",
    "Почему использование сверточных слоев может быть вычислительно выгодно в контексте глубинного обучения?\n",
    "\n",
    "**Замечание**: Требуется привести аналитическое решение (можно не программировать)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define \n",
    "$$\n",
    "\\vec{x} = (x_0, \\ldots, x_{m-1}), \\\\\n",
    "\\vec{w} = (w_{-p}, \\ldots, w_0, \\ldots, x_p), \\\\\n",
    "\\vec{y} = (y_0, \\ldots, y_{m-1}).\n",
    "$$\n",
    "Rewrite given equation as follows\n",
    "$$\n",
    "y_j = \\sum \\limits_{k=-p}^p x_{j - k} w_k. \n",
    "$$ \n",
    "\n",
    "___1 case___\n",
    "Note that\n",
    "$$\n",
    "\\dfrac{\\partial y_{j + p}}{\\partial x_j} = w_p, ~ \\dfrac{\\partial y_{j + p - 1}}{\\partial x_j} = w_{p - 1}, \\ldots \\dfrac{\\partial y_{j - p + 1}}{\\partial x_j} = w_{-p+1}, ~ \\dfrac{\\partial y_{j - p}}{\\partial x_j} = w_{-p}\n",
    "$$\n",
    "and there is no dependence on index $j.$\n",
    "\n",
    "Thus, \n",
    "$$\n",
    "\\dfrac{\\partial y_{j + k}}{x_j} = w_k,\\\\\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial x_j} = \\sum \\limits_{k=-p}^p \\dfrac{\\partial \\mathcal{L}}{\\partial y_{j + k}} \\dfrac{\\partial y_{j + k}}{\\partial x_j} = \\sum \\limits_{k=-p}^p \\dfrac{\\partial \\mathcal{L}}{\\partial y_{j + k}} w_k.\n",
    "$$\n",
    "___2 case___\n",
    "Note that\n",
    "$$\n",
    "\\dfrac{\\partial y_{j}}{\\partial w_p} = x_{j-p}, ~ \\dfrac{\\partial y_{j}}{\\partial x_{p-1}} = w_{j-p+1}, \\ldots \\dfrac{\\partial y_{j}}{\\partial w_{-p + 1}} = x_{j+p-1}, ~ \\dfrac{\\partial y_{j}}{\\partial w_{-p}} = x_{j+p}.\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "$$\n",
    "\\dfrac{\\partial y_{j}}{w_k} = x_{j-k},\\\\\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial w_k} = \\sum \\limits_{j=0}^{m-1} \\dfrac{\\partial \\mathcal{L}}{\\partial y_{j}} \\dfrac{\\partial y_{j}}{\\partial w_k} = \\sum \\limits_{j=0}^{m-1} \\dfrac{\\partial \\mathcal{L}}{\\partial y_{j}} w_{j-k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBJ8r-JL86mw"
   },
   "source": [
    "# Квадратичная форма (4 балла)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YE5rIeh86mx"
   },
   "source": [
    "Пусть имеется функция:\n",
    "$$\n",
    "Q = Q(\\vec{x}) = \\vec{x}^\\top W^\\top W \\vec{x}\n",
    "$$\n",
    "\n",
    "Вычислить производные, предложить алгоритм для backpropagation через данное преобразование:\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial \\vec{x}}, \\; \\frac{\\partial Q}{\\partial W}\n",
    "$$\n",
    "\n",
    "**Замечание**: Требуется привести аналитическое решение (можно не программировать)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhpAfaQh86my"
   },
   "source": [
    "Let's $W \\in \\mathbb{R}^{m \\times n}$ be a matrix and $\\vec{x} \\in \\mathbb{R}^n$ be a column-vector. Consider the map $Q: \\mathbb{R}^n \\rightarrow \\mathbb{R}.$\n",
    "\n",
    "___1 case___\n",
    "$$\n",
    "\\dfrac{\\partial Q(\\vec{x})}{\\partial \\vec{x}} = \\left(\\dfrac{\\partial Q}{\\partial x_1}, \\ldots, \\dfrac{\\partial Q}{\\partial x_n} \\right)^T.\n",
    "$$\n",
    "Rewrite the next equation $Q(\\vec{x}) = \\vec{x}^T W^T W^T \\vec{x} = \\langle \\vec{x}, W^T W \\vec{x} \\rangle.$ Now we can differentiate the dot product:\n",
    "$$\n",
    "\\dfrac{\\partial Q(\\vec{x})}{\\partial \\vec{x}} = 2 W^T W \\vec{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldJyUlx-86m0"
   },
   "source": [
    "___2 case___\n",
    "$$\n",
    "\\dfrac{\\partial Q(\\vec{x})}{\\partial W} = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial Q}{\\partial w_{11}} & \\ldots & \\dfrac{\\partial Q}{\\partial w_{1n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial Q}{\\partial w_{m1}} & \\ldots & \\dfrac{\\partial Q}{\\partial w_{mn}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Rewrite the next equation $Q(\\vec{x}) = \\vec{x}^T W^T W^T \\vec{x} = \\langle W \\vec{x}, W \\vec{x} \\rangle = \\sum \\limits_{i = 1}^m [W \\vec{x}]_i^2 = \\sum \\limits_{i=1}^m \\left(\\sum \\limits_{j=1}^n w_{ij} x_j \\right)^2.$ Now we can differentiate this sum as follows:\n",
    "$$\n",
    "\\dfrac{\\partial Q(\\vec{x})}{\\partial w_{ij}} = 2 x_j (w_{i1}x_1 + \\ldots + w_{in}x_n).\n",
    "$$\n",
    "Thus, \n",
    "$$\n",
    "\\dfrac{\\partial Q(\\vec{x})}{\\partial W} = 2 W \\vec{x} \\cdot \\vec{x}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Idea!__ \n",
    "* 1 Remember $\\vec{x}$ while running forward propagation like $\\dfrac{\\partial Q(\\vec{x})}{\\partial \\vec{x}}.$ \n",
    "* 2 Calculate loss function + gradient w.r.t. W.\n",
    "* 3 Substitute $\\vec{x}$ from the forward propagation in $\\dfrac{\\partial Q(\\vec{x})}{\\partial W}$  while running backward propagation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
